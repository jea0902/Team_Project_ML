{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c8b9ca-73f7-4ac9-895d-f93a6b7ec394",
   "metadata": {},
   "source": [
    "'이동 통신사 이탈 고객 예측 및 개선방안' 팀 미니 프로젝트 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1f777c-a453-4cf7-a663-12dad898148b",
   "metadata": {
    "tags": []
   },
   "source": [
    "학습했던 걸 기억하는 것이 목표인 첫 회고라 조금 길고 장황하게 작성.\n",
    "\n",
    "\n",
    "### 01. 분석 배경 :\n",
    "\n",
    "kaggle에서 2022년 2분기 캘리포니아 지역 모통신사의 고객 정보 데이터셋을 발견하고, 한국의 이동 통신사 기업들은 미국의 이동 통신사 기업들을 본따 만들었다고 해서, 한국 이동 통신사 기업을 대신해서 미국 데이터 셋을 통해 기업인의 입장에서 데이터를 분석해서 솔루션을 내보는 경험을 해보고싶어 분석해봤다.\n",
    "\n",
    "\n",
    "### 02.데이터 파악 및 전처리 :\n",
    "\n",
    "데이터는 7043 X 38로 이루어져 있고, 개별 고객들의 정보와 이탈 여부로 만들어져 있었다.\n",
    "\n",
    "먼저, 각 컬럼들의 의미는 kaggle에 나와있는 정보와 파이썬에서 profile_report로 알아보았고, 결측치와 이상치 그리고 중복값들을 처리하려고 했다. 팀원들과 나중에 데이터를 통합하기 위해 데이터들의 이름을 rename으로 통일시켜주었고, 데이터값들의 의미를 더 직관적으로 이해하기 위해 값의 이름들도 바꾸어주었다.\n",
    "\n",
    "중복값은 아예 없었고, 결측치들은 14개 컬럼에서 대량으로 발견되어 나중에 통계를 잡을 수 있는 컬럼들 중 연속형 변수들은 전부 평균값으로 처리해주었고, 비연속형 변수들은 특정 컬럼의 값에 종속되어있어 종속된 값들이 null이면 No Phone Service, No Internet Service라고 \n",
    "결측치들을 한꺼번에 통일해서 처리해주었다.\n",
    "\n",
    "\n",
    "### 03. EDA 탐색적 데이터 분석과 시행착오 :\n",
    "\n",
    "우리팀은 이런 데이터 분석 프로젝트가 처음이라 데이터를 정확하게 이해해야할 필요성을 많이 느끼진 못했었다.\n",
    "그래서 다짜고짜 데이터 시각화를 해보고 인사이트를 찾아보자라고 시작을 했지만,\n",
    "정확히 데이터를 이해하지 못했었기에 힘들게 한줄 한줄 짜왔던 파이썬 코드가 전부 그저 의미없는 시행착오가 되었었다.\n",
    "그렇게 EDA라는 '탐색적 데이터 분석'이라는 데이터 분석 과정이 얼마나 중요한 것인지 체감하게 되었다.\n",
    "왜냐면 모든 팀원이 이 데이터를 제대로 이해한 사람이 없기에 기획 절차를 명확히 잡고 나아간 것이 아니였고,\n",
    "소통이 안되니 시간만 낭비하는 시행착오를 거친 것이었다.\n",
    "\n",
    "실제로, 실무에서 어떤 프로젝트든 기획단계에서 이해가 잘 되고, 관계자들의 소통이 잘 된 프로젝트들은 일의 제작 및 수행과정이 그렇지 못한 프로젝트보다 훨씬 수월한 편이라고 한다.\n",
    "\n",
    "EDA는 이렇게 말한다. 일을 진행하면서도 지속적으로 기획단계의 의도를 피드백하고 소통하는 팀은 프로젝트 결과의 질을 높게 만들 수 있다고.\n",
    "\n",
    "뿐만 아니라 가장 중요한 자원 중 하나인 시간을 절약할 수 있었을 테지만, 이를 몰라서 시간을 많이 낭비했었다.\n",
    "\n",
    "팀원들과 데이터를 좀 더 이해하고나서 기획 단계를 다시 잡아보자고 한 후, 진행해보니 훨씬 수월하게 진행할 수 있었고, 소통도 더 활발해질 수 있었다.\n",
    "\n",
    "그래서 무엇을 어떻게 데이터를 시각화해봐야 할지 팀원들과 의견을 공유하고 데이터 시각화를 시작했다.\n",
    "\n",
    "\n",
    "### 04. 데이터 시각화 : \n",
    "\n",
    "결론적으로 우리는 이탈한 고객이 아니지만, 이탈한 고객들을 예측할 수 있는 모델을 만들고 싶었고, 이들을 높은 확률로 예측헤서 이들의 특징을 뽑아 내고, 이탈 고객들이 되는 것을 방지할 수 있는 솔루션을 내고 싶었다.\n",
    "\n",
    "그렇기에 Y칼럼(종속 변수)으로 '이탈 어부'를 잡고, 나머지를 X컬럼(독립 변수)으로 잡아 변수마다 이탈여부를 그룹화시켜 상관관계를 시각화해봤다.\n",
    "\n",
    "값이 Unique하지 않은 연속형 변수들은 전부 Boxplot으로 이상치를 확인하기 위해 그래프로 시각화 해보니, 이상치가 이상치가 아니고, 보통 값이 No인 값들이 대다수이고, 오히려 중요한 값들이 소수라 이상치로 잡혀있기에 전처리 단계에서 정규표준화만 시켜놓은 상태였다. 그렇기에 그래프로 시각화해서 봐도 인사이트를 찾아내기 쉽지 않아 describe로 표로 볼 수 있게 만들어 두었고,\n",
    "\n",
    "그리고 고객들을 대상으로 이탈한 이유를 설문조사한 컬럼은 따로 빼두어 그래프로 만들고, 각각 그래프 위에 각 비율을 적고, 이탈한 사람들이 말하는 이유를 정리해 두었다.\n",
    "\n",
    "이외의 비연속형 변수들과, 연속형 변수더라도 Unique한 값들의 갯수가 적은 변수들을 Countplot으로 전부시각화 해보니, 고객들이 이탈하는 패턴을 발견할 수 있었다. 그래서 이 X Feature들로 상관관계를 연상시킬 수 있다고 보여지는 패턴들을 각각 인사이트로 기록해 두었다. 한 예시로 tenure_in_months 라는 통신사 가입 기간을 1년 단위로 끊어서 이탈 여부를 그룹화 시켜 각각 이탈률을 시각화 한 것이다.\n",
    "\n",
    "Countplot보다 더 직관적으로 볼 수 있을 법한 X Feature들은 Pie Chart로 그려서 시각화 해두었고,\n",
    "\n",
    "\n",
    "### 05. 상관계수 :\n",
    "애초부터 X feture들끼리 상관관계가 있었던 변수들을 가려내기 위해 상관계수로도 표현해보았다.\n",
    "\n",
    "모든 컬럼들을 상관계수로 표현해서 보고싶었기에 모든 변수들을 연속형 변수로 바꿀 필요가 있었다. 그래서 원핫 인코딩(값이 0 or 1)이 아니라 레이블 인코딩으로 팀원들과 결정해서 Unique한 값들의 갯수만큼 0,1,2,3,4,5 --- 으로 잡아서 레이블 인코딩을 시켜 주었다.\n",
    "\n",
    "상관관계가 있는 변수들끼리만 머신러닝 모델을 돌려보는 것도 새로운 인사이트를 도출해볼 수 있는 시행착오라고 생각해봤기 때문에 상관계수를 시각화 해보았고, 강한 상관관계는 상관계수가 0.5이상이라는 기준으로 잡아 분류해 두었다.\n",
    "\n",
    "시각화 하다가 데이터 자체가 Y 변수인 이탈과 그리 관계가 없는 컬럼들은 팀원들과의 소통 후에 drop을 해주었다. 안그래도 Feature가 많기 때문에 나중에 헷갈리지 않도록 미리 제거해둔 것이다.\n",
    "\n",
    "후에 의미있는 X Feature들로만 머신러닝 모델을 돌려보고, 이탈 고객을 예측하기 위함이었다.\n",
    "\n",
    "\n",
    "### 06. 이탈 고객 예측을 위한 머신러닝 모델링(테스트 데이터 세트에서 패턴을 찾거나 이를 근거로 결정을 내릴 수 있는 프로그램) :\n",
    "\n",
    "고객 분류에서 joined라는 가입기간이 3개월 이하인 고객들은 극소수기에 제거해주어, 이탈 고객인지 이탈 고객이 아닌지를 이분 분류로 전처리를 해 두었다. \n",
    "\n",
    "이전 단계에서 의미없는 컬럼들을 제거하고 나니, 총 30개의 컬럼이 있었다.\n",
    "\n",
    "이제 데이터들을 X_train, X_test, y_train, y_test로 분류해주고,\n",
    "\n",
    "금액과 관련된 컬럼들은 따로 표준화 작업을 전처리로 해 주었다.\n",
    "\n",
    "최적화 작업과 교차검증을 해주기 전에,\n",
    "\n",
    "(예측 점수가 50% 미만의 확률을 기록한 기법들은 맞지 않다고 판단해 미리 제거해두었다.)\n",
    "\n",
    "#### 06-1. 각 모델링을 선택한 이유 : \n",
    "\n",
    "로지스틱 회귀는 두 데이터 요인 간의 관계를 찾는 데이터 분석 기법이고, \n",
    "점수는 84점이 나왔다.\n",
    "\n",
    "의사결정 트리는 여러 독립 변수 중 하나의 독립 변수를 선택하고,그 독립 변수에 대한 기준값(threshold)을 정하고 분류해나가는 분석 기법이고, 82점이 나왔다.\n",
    "\n",
    "앙상블 기법은 여러 약한 분류기법들을 결합하여 강 분류기법으로 만드는 방법으로,\n",
    "\n",
    "부스팅과 배깅이 있는데,\n",
    "배깅의 대표적인 모델인 랜덤 포레스트, 부스팅의 대표적인 모델인 그래디언트 부스팅이 있다고 한다.\n",
    "그래디언트의 변형 모델로는 XGBoost, LightGBM 등이 있다.\n",
    "\n",
    "랜덤 포레스트는 하나의 결과에 도달하기 위해 여러 의사결정트리의 출력을 결합한 분석 기법이고, 86점이 나왔다.\n",
    "\n",
    "그래디언트 부스팅 분류기법은 이전 학습의 결과에서 나온 오차를 다음 학습에 전달해 이전의 오차를 점진적으로 개선하는 부스팅 기법이자 앙상블 기법이다. 88점이 나왔다.\n",
    "\n",
    "XGBoost는 그래디언트 부스팅의 변형 기법으로, 다양한 데이터 형식, 관계, 분포 및 미세 조정할 수 있는 다양한 하이퍼 파라미터를 강력하게 처리할 수 있는 기법이다. 87점이 나왔다.\n",
    "\n",
    "LightGBM도 그래디언트 부스팅의 변형 기법으로, 예측에 틀린 부분에 가중치를 더해가면서 진행하는 기법이다. 88점이 나왔다.\n",
    "\n",
    "이외에도 스태이킹 88점, 배깅 87점, 보팅이 88점이 나왔다.\n",
    "\n",
    "이 모델들에 좀 더 평균적이고, 일반화된 성능을 알아보기 위해\n",
    "K-fold 교차검증을 해봤다.\n",
    "\n",
    "이때 정확도(Accuracy)\n",
    ", ROC-AUC 커브 곡선, Precision(정밀도), Recall(재현율 or 민감도)\n",
    ", f1 Score(정밀도와 재현율을 같이 보기 위함)\n",
    "라는 이름들의 검증 점수들을 전부 넣어 보았는데,\n",
    "이 중에서 AUC-ROC 곡선은 여러 분류기들을 바르게 예측\n",
    "(1로)하는 비율과 틀리게 예측(0으로)하는 비율을 분류기마다 시각화할 수 있었기에 ROC-AUC 커브 곡선을 최종 검증 점수로 정하고, 좌상단으로 가장 많이 치우친 그래프를 갖는 모델을 골라내려고 했다.\n",
    "\n",
    "### 6-2. 하이퍼 파리미터 튜닝 : \n",
    "\n",
    "최종적으로 모델을 선정하기 전에,\n",
    "데이터 분석가가 최적의 결과를 위헤 모델성능을 조정하는 방법으로, 각 모델링들에 한꺼번에 전부 하이퍼 파라미터를 조정하기 위해 \n",
    "먼저, 모델을 생성하는 함수 - 하이퍼 파라미터를 조정하는 함수(모델에게 가장 적합한 파라미터를 찾는 그리드 서치를 함수에 넣어) - 모델별 평가를 하는 함수 - ROC 커브 곡선에 대입하는 함수\n",
    "순으로 생성해서 모델링을 돌려보았다.\n",
    "\n",
    "### 06-3. 각 모델링별 최종 점수(ROC-AUC 검증 점수) : \n",
    "\n",
    "먼저, ROC-AUC 커브 곡선에 대한 이해가 필요했다.\n",
    "이 곡선은 다양한 입계값에서 모델의 분류 성능에 대한 측정 그래프로\n",
    "ROC = 모든 임계값에서 분류 모델의 성능을 보여주는 그래프이고,\n",
    "AUC = ROC 곡선 아래 영역이다.\n",
    "\n",
    "AUC가 높다는 사실은 클래스를 구별하는 모델의 성능이 훌륭하다는 것을 의미하고,\n",
    "임상에서 ROC-AUC곡선은 정상인 및 환자 클래스를 구분하는 모델의 성능평가로 흔하게 사용된다.\n",
    "ROC 곡선은 TPR(=민감도)이 y축에 있고, FPR(=1-Specificity)이 x축으로 그려진다.\n",
    "y축 민감도란 실제로 질병이 있을 때, 검사 결과가 양성인 경우이다.(예측을 맞춘 경우)\n",
    "x축은 예측이 틀린 경우이다.\n",
    "\n",
    "우수한 분류 모델은 AUC값이 1에 가깝고, ROC 곡선이 왼쪽 위로 높이 올라가 있을 수록 우수하다는 것 그래프이다.\n",
    "\n",
    "\n",
    "4. Finding good model (테스트 세트)\n",
    "에 있는 훈련세트로 점수낸거랑 검증세트로 점수 뽑아낸 것.\n",
    "\n",
    "- 로지스틱 회귀\n",
    "훈련 점수 : 91.9점\n",
    "테스트 점수 : 82.7점\n",
    "\n",
    "- 배깅\n",
    "훈련 점수 : 93.8점\n",
    "테스트 점수 : 81.5점\n",
    "\n",
    "- 랜덤 포레스트\n",
    "훈련 점수 : 93.8점\n",
    "테스트 점수 : 81.5점\n",
    "\n",
    "- 그래디언트 부스팅\n",
    "훈련 점수 : 96.4점\n",
    "테스트 점수 : 83.6점\n",
    "\n",
    "- XGBoost\n",
    "훈련 점수 : 95.2점\n",
    "테스트 점수 : 83.5점\n",
    "\n",
    "- SVC(서포트 벡터 머신)\n",
    "훈련 점수 : 77.9점\n",
    "테스트 점수 : 82.9점\n",
    "\n",
    "- 의사결정트리\n",
    "훈련 점수 : 91점\n",
    "테스트 점수 : 78.3점\n",
    "\n",
    "- LightGBM\n",
    "훈련 점수 : 95.3점\n",
    "테스트 점수 : 83.6점\n",
    "\n",
    "### 06-4. Finding Good Model (테스트 세트)\n",
    "\n",
    "다중 모델 평가를 위한 교차 검증(cross-validation)을 수행하는 과정을 훈련세트와 테스트세트 모두 검증해서 ROC AUC,\tAccuracy, Precision, Recall, f1 Score 점수를 뽑아내었다.\n",
    "\n",
    "\n",
    "### 07. ROC 커브 곡선 시각화\n",
    "\n",
    "모델링에, 최적화에, 교차 검증까지 끝내고, 각 모델들을 ROC 커브 곡선으로 시각화까지 해주었다.\n",
    "어떤 모델이 가장 우수한 분류모델일지 시각화로 보고싶었기 때문이다.\n",
    "\n",
    "\n",
    "### 08. SHAP for feature importance 샤플리 값으로 feature 중요도를 뽑다 :\n",
    "\n",
    "\n",
    "특성 중요도가 필요한 이유\n",
    "\n",
    "첫번째로,\n",
    "앙상블 모형은 많은 모델들이 기본적으로 Tree 기반으로 이루어진다. 동시에, 이 Tree기반의 앙상블들은 전반적으로 우수한 성능을 내는 모델들이라고도 알려져 있다. \n",
    "\n",
    "하지만, 앙상블 기법을 사용하면서 Decision Tree들의 결합과 반복되는 학습과정에서 Decision Tree의 뛰어난 직관성이 사라진다. 변수 및 모델의 설명력을 위해서 Tree를 사용하는데 성능을 높이려고 앙상블 기법을 추가하다 보니, 원래의 목적을 잃는 것이다. 이처럼 정확성(Accuracy)과 설명력은 모델 선택에 있어서 trade-off 관계가 존재한다. \n",
    "\n",
    "그러다 보니, 모델에 대한 해석이 필요할 때(target에 어떤 변수가 영향을 미치는지) Linear regression과 Decision Tree를 선택하고, 예측 및 분류의 성능이 중요할 때는 Ensemble과 Neural Network와 같은 복잡한 모델을 사용하게된다. \n",
    "\n",
    "물론 Ensemble learning에서도 기본적으로 라이브러리 내 내장함수를 통해서 변수의 중요도(Importance), 즉 중요 feature를 추출할 수 있는 알고리즘을 제공한다. 그 형태는 아래의 그림과 같다. (특성 중요도 사진)\n",
    "\n",
    "이렇게 특성 중요도를 내림차순으로 보여주는 데, 하지만, 실제로 영향력의 방향성은 제공하지 않는다. 실제로 Age라는 중요한 변수가 positive 영향력을 가질지, negative 영향력을 가질지 모른다는 것이다. \n",
    "\n",
    "두번째로,\n",
    "과대적합을 상쇄하려고 하이퍼 파라미터를 조정했음에도 불구하고, 모든 X feature들을 넣고 점수를 돌리니 대부분 과대적합이 나왔다. 과대적합을 줄이는 방법 중에 들어가는 X feature 수를 줄이는 방법이 있다. 좀 더 예측력이 좋고 검증된 모델링을 사용하려면 너무 많은 X feature를 사용하는 것은 문제라고 판단해, feature를 줄이려고 했는데 어떤 feature가 예측에 영향을 미치는지 모르기에 단순 상식으로 나눌 수가 없었다.\n",
    "그래서 특성 중요도 검사를 해보기로 했다.\n",
    "\n",
    "위에서 짧게나마 보았던 중요도를 측정하는 기준은 Xgboost에서는 Weight, Cover, Gain을 기준으로 판단된다. 이 기준에 대한 뜻은 아래와 같다.\n",
    "\n",
    " Weight : 변수 별 데이터를 분리하는데 쓰인 횟수 \n",
    "\n",
    " Cover : 해당 변수로 분리된 데이터의 수\n",
    "\n",
    " Gain : Feature을 사용했을 때 줄어드는 평균적인 training loss\n",
    "\n",
    " 이러한 기준을 통해서 feature의 importance가 구분되는 것이다. \n",
    "\n",
    "\n",
    "문제가 한 가지 더 발생한다. 위의 기준으로 내린 각각의 중요한 변수를 봐보면, 각 Weight/Cover/Gain마다 너무 상이한 변수들을 중요하다고 말한다. 그렇기에 \"그래서 뭐가 중요한 변수인가?\"라는 질문에 쉽게 대답을 할 수가 없다. 게다가 처음 말했던 각각의 기준에 봤을 때도 어떤 변수가 어떤 방향으로(positive or negative)로 영향일 미치는지 판단을 할 수가 없다. \n",
    "\n",
    "물론 아예 안 쓰이는 변수를 걸러낼 수는 있다. 그리고 이 Feature imporatance에도 좋고 나쁨의 기준은 있다. 그것은 바로 Consistency(일관성)이다. 즉 특정 feature를 중요하다고 판단하여 영향이 많이 가도록 모델을 수정하였다면, 중요도 측정 시 해당 feature의 중요도는 줄어들지 않아야 한다는 것이다. 쉽게 말해서, 한 앙상블 모델에 쓰인 변수가 중요하다고 판단이 되었다면, 다른 앙상블 모델을 쓰더라도 중요해야 한다는 것이다. 하지만 문제는 대부분의 feature importance 지표는 inconsistency하다는 것이다.\n",
    "\n",
    "이러한 문제들을 보완하기 위해서 나온 방법이 바로 Shap Value이다. \n",
    "\n",
    "Shapley Value에 대해 알기 위해서는 게임이론에 대해 먼저 이해해야한다. 게임이론이란 우리가 아는 게임을 말하는 것이 아닌 여러 주제가 서로 영향을 미치는 상황에서 서로가 어떤 의사결정이나 행동을 하는지에 대해 이론화한 것을 말한다.\n",
    "\n",
    "게임이론을 바탕으로 하나의 특성 중요도를 알기위해 여러 특성들의 조합을 구성하고 해당 특성의 유무에 따른 평균적인 변화를 통해 얻어낸 값이 바로 shapley value이다.\n",
    "\n",
    "그렇게 랜덤포레스트 모델부터 그래디언트 부스팅, LGBM 모델별 특성 중요도를 내림차순으로 시각화 해봤습니다.\n",
    "\n",
    "\n",
    "### 09. 각 모델별 특성 중요도 상위 뽑기\n",
    "먼저,\n",
    "Permutation Importance 기법은, 모델 예측에 가장 큰 영향을 미치는 Feature 를 파악하는 방법입니다.\n",
    "Permutation Importance 는 모델 훈련이 끝난 뒤에 계산되며,\n",
    "훈련된 모델이 특정 Feature 를 안 썼을 때, 이것이 성능 손실에 얼마만큼의 영향을 주는지를 통해,\n",
    "그 Feature 의 중요도를 파악하는 방법입니다.\n",
    "또한, 어떤 모델이든 적용할 수 있습니다.\n",
    "\n",
    "permutation Importance 기법으로 먼저 특성별 영향도를 뽑아봤고,\n",
    "\n",
    "모델별로 특성 중요도를 시각화해서 상위 특성들을 더 보기 쉽게 표현해봤다.\n",
    "\n",
    "특성 중요도로 뽑은 특성들 상위 9개\n",
    "\n",
    "['contract', 'tenure_in_months', 'monthly_charge', 'number_of_referrals', 'age', 'number_of_dependents', 'online_security'] \n",
    "위 특성들로만 ROC까지 모델링을 돌렸을 때 점수는\n",
    "age 이후부터는 넣거나 빼도 정확도 점수는 92점이 고정이었다.\n",
    "\n",
    "특성 중요도 기반으로 모델 성능평가까지 거쳤으니 점수 기반으로 가장 좋았던 모델을 하나 선정했습니다.\n",
    "\n",
    "\n",
    "### 10. 모델 선정 그래디언트 부스팅\n",
    "\n",
    "모델 선정 이유 : 예측 점수가 항상 상위권이었고, 분류와 회귀 영역에서 뛰어난 예측 성능을 발휘했기에 선정했습니다.\n",
    "\n",
    "\n",
    "### 11. 추가 인사이트 제시\n",
    "\n",
    "인사이트 제시는 시각화를 통해서만 가능했기에 여러 세부 조건들로 더 시각화를 진행해봤습니다.\n",
    "\n",
    "연속형 변수였던 tenure_in_months(가입유지 기간)에서 기간별로 짤라서 시각화 시켜주어봤고,\n",
    "이탈 고객으로 인한 수입감소를 알아보기 위해 groupby를 사용하여 describe()로 count와 mean 등 quantile 값들을 확인했습니다.\n",
    "이를 통해, 고객 1명당 79.5달러씩 감소되며 이를 기존 고객으로부터 채우려면 얼마가 필요한지도 계산해보았고,\n",
    "\n",
    "신규 가입자들의 이탈률, 서비스 가입 종류별 이탈률, 가입 유지 기간별, 약정별 고객 이탈률과,\n",
    "가입 유지기간과 약정을 조건으로 묶어서 시각화도 해보았습니다.\n",
    "그리고 인구 통계학적 특성에 따른 고객 이탈률과 추천인 유무에 다른 이탈률, 부양가족 수에 따른 이탈률과 하이밸류 고객들의 이탈률도 시각화 해보았습니다.\n",
    "\n",
    "신기했던 점은 계속 세부 조건으로 묶어서 보니까 새로운 관점과 인사이트가 확장되어 간다는 점이었습니다.\n",
    "\n",
    "\n",
    "### 12. 아쉬웠던 점 :\n",
    "    \n",
    "초반에 EDA 대로 방향을 잘 잡지 못했던 점, 시각화에 너무 많은 시간을 쏟은 점,\n",
    "\n",
    "또한, ppt 작성 시간이 늦을 거 같아 학습이 중요 포인트였는데도 마감을 위해 혼자 ppt 만드는 데 시간이 너무 많이 들었기에 완성도가 떨어졌다는 점이 문제였다고 생각합니다.\n",
    "    \n",
    "그리고 시각화와 모델링은 계속 반복적으로 깊게 파보면서 공부를 더 해봐야할 것 같다는 점도 배운 점이었습니다.\n",
    "    \n",
    "마지막으로 팀프로젝트에서 역시 중요한 건 분업이지만, 첫 프로젝트였던만큼 희생플레이가 아니라 내가 깊게 배우기 위한 연습이었다고 좀 더 가볍게 생각하면서 했다면 더 나은 퀄리티를 만들 수 있었을 것 같습니다.\n",
    "특히, 모델링 부분은 계속해서 봐야할 정도로 공부가 부족했고, 이해하기 위해서는 시간이 더 필요하다고 느꼈습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b8ab88-ebb9-4f80-867d-e0c17d96ed10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
